{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gymnasium as gym\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio\n",
    "\n",
    "from functions.huggingface_course import record_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "img = torch.from_numpy(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compressed_render(img: torch.TensorType) -> torch.TensorType:\n",
    "    img = img.float().mean(dim=2)\n",
    "    compressed_img = F.interpolate(img.view(1, 1, *img.shape), size=(48, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "    return compressed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff14127510>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(get_compressed_render(img).squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = env.observation_space.shape[0]\n",
    "a_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_stack(env, stack_size: int = 3):\n",
    "    image_list = []\n",
    "    _ = env.reset()\n",
    "    for _ in range(stack_size): \n",
    "        render = env.render()\n",
    "        render = get_compressed_render(torch.from_numpy(render).float()).squeeze(0)\n",
    "\n",
    "        image_list.append(render)\n",
    "\n",
    "        _, _, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "    image_stack = torch.stack(image_list, dim=1).to(device)\n",
    "\n",
    "    return image_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, a_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(4, 2)\n",
    "        # Calculate the size here\n",
    "        self.fc1 = nn.Linear(4480, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.flatten()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "    def act(self, render):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        probs = self.forward(render)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(-0.0244, device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Policy(a_size).to(device)\n",
    "\n",
    "img_stack = initialize_stack(env)\n",
    "policy.act(img_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queue_new_render(env, img_stack: torch.TensorType):\n",
    "    render = env.render()\n",
    "    render = get_compressed_render(torch.from_numpy(render).float()).squeeze(0)\n",
    "\n",
    "    new_stack = img_stack.clone()\n",
    "\n",
    "    new_stack[:, :-1, :, :] = new_stack.clone()[:, 1:, :, :]\n",
    "    new_stack[:, -1, :, :] = render \n",
    "\n",
    "    return new_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stack = queue_new_render(env, img_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor(-0.0326, device='cuda:0', grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.act(new_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        img_stack = initialize_stack(env)\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(img_stack)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            img_stack = queue_new_render(env, img_stack)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "\n",
    "        # Given this formulation, the returns at each timestep t can be computed\n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
    "        # to avoid computing them multiple times)\n",
    "\n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "\n",
    "\n",
    "        ## Given the above, we calculate the returns at timestep t as:\n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
    "        ## if we were to do it from first to last.\n",
    "\n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(rewards[t] + gamma * disc_return_t) # TODO: complete here\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "\n",
    "        policy_loss = torch.cat([loss.unsqueeze(0) for loss in policy_loss]).sum()\n",
    "\n",
    "        # Line 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 128,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 0.95,\n",
    "    \"lr\": 2e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(\n",
    "    cartpole_hyperparameters[\"action_space\"],\n",
    ").to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 9.49\n",
      "Episode 200\tAverage Score: 9.29\n",
      "Episode 300\tAverage Score: 9.58\n",
      "Episode 400\tAverage Score: 9.27\n",
      "Episode 500\tAverage Score: 9.58\n",
      "Episode 600\tAverage Score: 9.67\n",
      "Episode 700\tAverage Score: 10.02\n",
      "Episode 800\tAverage Score: 9.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m reinforce(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cartpole_policy,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cartpole_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[1;32m/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Line 8: PyTorch prefers gradient descent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mif\u001b[39;00m i_episode \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    cartpole_policy,\n",
    "    cartpole_optimizer,\n",
    "    cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "    cartpole_hyperparameters[\"max_t\"],\n",
    "    cartpole_hyperparameters[\"gamma\"],\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a replay video of the agent\n",
    "    :param env\n",
    "    :param Qtable: Qtable of our agent\n",
    "    :param out_directory\n",
    "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    state = env.reset()[0]\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    terminated = False \n",
    "\n",
    "    while not terminated:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _ = policy.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)  # We directly put next_state = state for recording logic\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video(env, cartpole_policy, \"./data/cartpole_v0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
