{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gymnasium as gym\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio\n",
    "\n",
    "from functions.huggingface_course import record_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "img = torch.from_numpy(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compressed_render(img: np.ndarray) -> torch.TensorType:\n",
    "    img = img.float().mean(dim=2)\n",
    "    compressed_img = F.interpolate(img.view(1, 1, *img.shape), size=(48, 64), mode='bilinear', align_corners=False)\n",
    "\n",
    "    return compressed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff71bbe650>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(get_compressed_render(img).squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = env.observation_space.shape[0]\n",
    "a_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, a_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Calculate the size here\n",
    "        self.fc1 = nn.Linear(66560, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.flatten()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = state.float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m policy \u001b[39m=\u001b[39m Policy(a_size)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m policy\u001b[39m.\u001b[39mact(torch\u001b[39m.\u001b[39;49mtensor(env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49msample())\u001b[39m.\u001b[39;49mpermute(\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "policy = Policy(a_size).to(device)\n",
    "\n",
    "policy.act(torch.tensor(env.observation_space.sample()).permute(2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()[0]\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(torch.tensor(state).permute(2, 0, 1))\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity\n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "\n",
    "        # Given this formulation, the returns at each timestep t can be computed\n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order\n",
    "        # to avoid computing them multiple times)\n",
    "\n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "\n",
    "\n",
    "        ## Given the above, we calculate the returns at timestep t as:\n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed\n",
    "        ## if we were to do it from first to last.\n",
    "\n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(rewards[t] + gamma * disc_return_t) # TODO: complete here\n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "\n",
    "        print(policy_loss)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Line 8: PyTorch prefers gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 128,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 0.97,\n",
    "    \"lr\": 1e-3,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(\n",
    "    cartpole_hyperparameters[\"action_space\"],\n",
    ").to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>), tensor(0., device='cuda:0', grad_fn=<MulBackward0>)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m reinforce(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cartpole_policy,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cartpole_optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[1;32m/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     policy_loss\u001b[39m.\u001b[39mappend(\u001b[39m-\u001b[39mlog_prob \u001b[39m*\u001b[39m disc_return)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(policy_loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(policy_loss)\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Line 8: PyTorch prefers gradient descent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marc/Documents/Projects/huggingface_rl/handson2.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    cartpole_policy,\n",
    "    cartpole_optimizer,\n",
    "    cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "    cartpole_hyperparameters[\"max_t\"],\n",
    "    cartpole_hyperparameters[\"gamma\"],\n",
    "    100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a replay video of the agent\n",
    "    :param env\n",
    "    :param Qtable: Qtable of our agent\n",
    "    :param out_directory\n",
    "    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    state = env.reset()[0]\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    terminated = False \n",
    "\n",
    "    while not terminated:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action, _ = policy.act(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)  # We directly put next_state = state for recording logic\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_video(env, cartpole_policy, \"./data/cartpole_v0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
